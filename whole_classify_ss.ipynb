{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入库类和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import spectral\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = loadmat('H:\\data\\hyp_data.mat')['hyp_data']\n",
    "output_image = loadmat('H:\\data\\X.mat')['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.shape\n",
    "pca = PCA(n_components=30)\n",
    "pca.fit(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理第一维参数，提取信息，并进行kmeans， 计算欧式距离， 生成各像素空间特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以提取出高光谱数据的一个channel的数据 144*144\n",
    "def split_one_channel(A ,channel):\n",
    "    d = [0]*144\n",
    "    for i in range(144):\n",
    "        d[i] = input_image[i][:,channel-1]\n",
    "    e = np.vstack((d[i] for i in range(144)))\n",
    "    return e  \n",
    "# 参数说明\n",
    "# channel 通道\n",
    "# A 高光谱立方块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依据patch大小（Filter 大小）对矩阵进行镜像处理\n",
    "def mirror_matrix(A, fil):\n",
    "    pad = int((fil-1)/2)\n",
    "    for  i in range(1,pad+1):\n",
    "        up = np.array(A[2*i-1,:])\n",
    "        down = np.array(A[-2*i,:])\n",
    "        Matrix_up_down = np.vstack([up,A,down])\n",
    "        left = np.array(Matrix_up_down[:,2*i-1]).reshape(len(Matrix_up_down),1)\n",
    "        right = np.array(Matrix_up_down[:,-2*i]).reshape(len(Matrix_up_down),1)\n",
    "        Matrix_left_right = np.hstack([left,Matrix_up_down,right,])\n",
    "        A = Matrix_left_right\n",
    "    return A\n",
    "\n",
    "# 参数说明\n",
    "# n   array size\n",
    "# fil  filter size5*5\n",
    "# pad = (fil-1)/2  padding大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将镜像处理后的矩阵进行向量化操作\n",
    "def split_array(A,fil):\n",
    "    s = []\n",
    "    length = len(A)-fil+1\n",
    "    for i in range(length): # row\n",
    "        for j in range(length): # col\n",
    "            temp = [0]* fil\n",
    "            for z in range(fil):\n",
    "                temp[z] = A[i+z,j:j+fil]\n",
    "            com = np.concatenate([item for item in temp ])\n",
    "            s.append(com)\n",
    "    return s \n",
    "# 参数说明\n",
    "# A 镜像处理后的矩阵\n",
    "# fil  filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理first channel\n",
    "origin_matrix_one = split_one_channel(input_image,1)  # 取出第1个channel的信息\n",
    "mirror_matrix_one = mirror_matrix(origin_matrix_one,3) # 对第1个channel进行mirror操作\n",
    "split_list_one = split_array(mirror_matrix_one,3) # 将第一个channel进行向量化提取操作\n",
    "split_array_one = np.array(split_list_one) # 将list转化为numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans 操作 取K为5\n",
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(split_array_one)\n",
    "len(kmeans.labels_)\n",
    "K_centroid = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589.2696591076244\n"
     ]
    }
   ],
   "source": [
    "# 计算欧几里得距离\n",
    "def Euclidean(v1,v2):\n",
    "    return np.linalg.norm(v1-v2)\n",
    "\n",
    "vec1 = K_centroid[0]\n",
    "vec2 = split_array_one[0]\n",
    "print(Euclidean(vec1,vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,  28.64029311,   0.        , 273.84847726,\n",
       "         8.47646098])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算单个patch的空间信息\n",
    "def patch_spitial(v1, K_centroid):\n",
    "    distance_array = np.array([Euclidean(v1, centroid) for centroid in K_centroid]) #距离matrix\n",
    "    sums = np.sum(distance_array)\n",
    "    average = np.average(distance_array)\n",
    "    spitial_one_patch = np.maximum(0,average - distance_array) #依据公式求空间信息\n",
    "    return spitial_one_patch\n",
    "patch_spitial(vec2 ,K_centroid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对整个channel求解\n",
    "# test\n",
    "channel_spitial_list = list()\n",
    "for i in range(len(split_list_one)):\n",
    "    channel_spitial_list.append(patch_spitial(split_array_one[i], K_centroid))\n",
    "channel_spitial_array = np.array(channel_spitial_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20736, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对一个channel的求解进行向量化\n",
    "def channel_spitial(channel_matrix, K_centroid):\n",
    "    channel_spitial_list = list()\n",
    "    for i in range(len(channel_matrix)):\n",
    "        channel_spitial_list.append(patch_spitial(channel_matrix[i], K_centroid))\n",
    "    return np.array(channel_spitial_list)\n",
    "channel_spitial(split_list_one, K_centroid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20736, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_spitial_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对整个矩阵求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算144个channel的信息\n",
    "whole_origin_matrix_array = np.array([split_one_channel(input_image,i) for i in range(1,201)])\n",
    "whole_mirror_matrix_array = np.array([mirror_matrix(whole_origin_matrix_array[i],3) for i in range(200)])\n",
    "whole_split_matrix_array = np.array([split_array(whole_mirror_matrix_array[i],3) for i in range(200)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 20736, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_split_matrix_array.shape   # 200维的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_kmeans = [KMeans(n_clusters=5, random_state=0).fit(whole_split_matrix_array[i]) for i in range(200)]\n",
    "whole_K_centroid = [whole_kmeans[i].cluster_centers_ for i in range(200) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(whole_K_centroid).shape\n",
    "len(whole_K_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 20736, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_temp = list()\n",
    "for i in range(len(whole_K_centroid)):\n",
    "    whole_temp.append(channel_spitial(whole_split_matrix_array[i],whole_K_centroid[i]))\n",
    "whole_spitial = np.array(whole_temp)\n",
    "whole_spitial.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下边为测试提取到信息的代码  vstack合并列数不变,hstack合并行数不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"whole_spitial.npy\",whole_spitial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.load(\"whole_spitial.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 20736, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20736, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = np.hstack([s[i]for i in range(200)]) # sample\n",
    "s1.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = output_image.reshape(20736,1)   # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lb = np.hstack([s1,s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20736, 1001)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lb.shape   # sample with lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lab_fix_list = list()\n",
    "for i in range(len(sample_lb)):\n",
    "    if sample_lb[i,-1] != 0:\n",
    "        sample_lab_fix_list.append(sample_lb[i,:])\n",
    "sample_lab_fix = np.array(sample_lab_fix_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10366, 1001)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_lab_fix.shape    # 提取出需要分类的标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 归一化\n",
    "data_D = preprocessing.StandardScaler().fit_transform(sample_lab_fix[:,:-1])\n",
    "data_L = sample_lab_fix[:,-1]\n",
    "\n",
    "## 存储\n",
    "new = np.column_stack((data_D, data_L))\n",
    "new_ = pd.DataFrame(new)\n",
    "new_.to_csv('H:\\data\\specr_spi.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.54983922829582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['specr_spi.m']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分类\n",
    "data = pd.read_csv('H:\\data\\specr_spi.csv',header=None)\n",
    "data = data.as_matrix()\n",
    "data_D = data[:,:-1]\n",
    "data_L = data[:,-1]\n",
    "data_train, data_test, label_train, label_test = train_test_split(data_D,data_L,test_size=0.9)\n",
    "\n",
    "#clf = lgb.LGBMClassifier()\n",
    "clf = SVC()\n",
    "# clf = XGBClassifier(max_depth=20,learning_rate=0.1,n_estimators=200,silent=False,\n",
    "#                       objective='multi:softmax',\n",
    "#                       min_child_weight=1,\n",
    "#                      gamma=0.,\n",
    "#                        scale_pos_weight=1)\n",
    "clf.fit(data_train,label_train)\n",
    "pred = clf.predict(data_test)\n",
    "accuracy = metrics.accuracy_score(label_test, pred)*100\n",
    "print(accuracy)\n",
    "joblib.dump(clf,\"specr_spi.m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重构上边一部分代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import spectral\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import KMeans \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "\n",
    "input_image = loadmat('H:\\data\\hyp_data.mat')['hyp_data']\n",
    "output_image = loadmat('H:\\data\\X.mat')['X']\n",
    "\n",
    "# 可以提取出高光谱数据的一个channel的数据 144*144\n",
    "def split_one_channel(A ,channel):\n",
    "    d = [0]*144\n",
    "    for i in range(144):\n",
    "        d[i] = input_image[i][:,channel-1]\n",
    "    e = np.vstack((d[i] for i in range(144)))\n",
    "    return e\n",
    "\n",
    "# 依据patch大小（Filter 大小）对矩阵进行镜像处理\n",
    "def mirror_matrix(A, fil):\n",
    "    pad = int((fil-1)/2)\n",
    "    for  i in range(1,pad+1):\n",
    "        up = np.array(A[2*i-1,:])\n",
    "        down = np.array(A[-2*i,:])\n",
    "        Matrix_up_down = np.vstack([up,A,down])\n",
    "        left = np.array(Matrix_up_down[:,2*i-1]).reshape(len(Matrix_up_down),1)\n",
    "        right = np.array(Matrix_up_down[:,-2*i]).reshape(len(Matrix_up_down),1)\n",
    "        Matrix_left_right = np.hstack([left,Matrix_up_down,right,])\n",
    "        A = Matrix_left_right\n",
    "    return A\n",
    "\n",
    "# 将镜像处理后的矩阵进行向量化操作\n",
    "def split_array(A,fil):\n",
    "    s = []\n",
    "    length = len(A)-fil+1\n",
    "    for i in range(length): # row\n",
    "        for j in range(length): # col\n",
    "            temp = [0]* fil\n",
    "            for z in range(fil):\n",
    "                temp[z] = A[i+z,j:j+fil]\n",
    "            com = np.concatenate([item for item in temp ])\n",
    "            s.append(com)\n",
    "    return s \n",
    "\n",
    "\n",
    "def Euclidean(v1,v2):\n",
    "    return np.linalg.norm(v1-v2)\n",
    "\n",
    "# 计算单个patch的空间信息\n",
    "def patch_spitial(v1, K_centroid):\n",
    "    distance_array = np.array([Euclidean(v1, centroid) for centroid in K_centroid]) #距离matrix\n",
    "    sums = np.sum(distance_array)\n",
    "    average = np.average(distance_array)\n",
    "    spitial_one_patch = np.maximum(0,average - distance_array) #依据公式求空间信息\n",
    "    return spitial_one_patch\n",
    "\n",
    "# 对一个channel的求解进行向量化\n",
    "def channel_spitial(channel_matrix, K_centroid):\n",
    "    channel_spitial_list = list()\n",
    "    for i in range(len(channel_matrix)):\n",
    "        channel_spitial_list.append(patch_spitial(channel_matrix[i], K_centroid))\n",
    "    return np.array(channel_spitial_list)\n",
    "\n",
    "\n",
    "\n",
    "# 对dim个通道进行信息处理\n",
    "dim = 30 #维数\n",
    "fil = 5  #滤波器大小\n",
    "\n",
    "\n",
    "# pca 降维\n",
    "def pca(n_componets, input_image):\n",
    "    pca = PCA(n_components=n_componets)\n",
    "    pca.fit(input_image)\n",
    "    data_origin_pca = pca.transform(input_image)\n",
    "    return pca_result\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## 构造n个通道的向量\n",
    "def create_whole_vector(dim,fil，input_image):\n",
    "    whole_origin_matrix_array = np.array([split_one_channel(input_image,i) for i in range(1,dim+1)])\n",
    "    whole_mirror_matrix_array = np.array([mirror_matrix(whole_origin_matrix_array[i],fil) for i in range(dim)])\n",
    "    whole_split_matrix_array = np.array([split_array(whole_mirror_matrix_array[i],fil) for i in range(dim)])\n",
    "    return whole_split_matrix_array\n",
    "\n",
    "## 通过k-means求解 whole_K_centroid\n",
    "K =5 # K个中心\n",
    "def create_centroid(K,dim,whole_split_matrix_array):\n",
    "    whole_kmeans = [KMeans(n_clusters=K, random_state=0).fit(whole_split_matrix_array[i]) for i in range(dim)]\n",
    "    whole_K_centroid = [whole_kmeans[i].cluster_centers_ for i in range(dim) ]\n",
    "    return whole_K_centroid\n",
    "\n",
    "## 求空间信息\n",
    "def create_spe_spi(whole_K_centroid):\n",
    "    whole_temp = list()\n",
    "    for i in range(len(whole_K_centroid)):\n",
    "        whole_temp.append(channel_spitial(whole_split_matrix_array[i],whole_K_centroid[i]))\n",
    "    whole_spitial = np.array(whole_temp)\n",
    "    return whole_spitial\n",
    "\n",
    "## 可选操作，将spectral-spitial信息保存\n",
    "def save_spe_spi(whole_spitial):\n",
    "    np.save(\"whole_spitial.npy\",whole_spitial)\n",
    "## 可选操作，载入数据\n",
    "def load_spe_spi(name):\n",
    "    s = np.load(name)\n",
    "    return s\n",
    "\n",
    "## 合成sample_with_lables\n",
    "def sample_with_lables(s,dim):\n",
    "    sample = np.hstack([s[i]for i in range(dim)])\n",
    "    label = output_image.reshape(len(sample),1)\n",
    "    sample_lb = np.hstack([sample,label])\n",
    "    return sample_lb\n",
    "\n",
    "## 提取出需要分类的数据\n",
    "def need_claasify(sample_lb):\n",
    "    sample_lab_fix_list = list()\n",
    "    for i in range(len(sample_lb)):\n",
    "        if sample_lb[i,-1] != 0:\n",
    "            sample_lab_fix_list.append(sample_lb[i,:])\n",
    "    sample_lab_fix = np.array(sample_lab_fix_list)\n",
    "    return sample_lab_fix\n",
    "\n",
    "## 将数据归一化并且存储：\n",
    "def scaler_save(sample_lab_fix, path = 'H:\\data\\spec_spi.csv'):\n",
    "    ## 归一化\n",
    "    data_D = preprocessing.StandardScaler().fit_transform(sample_lab_fix[:,:-1])\n",
    "    data_L = sample_lab_fix[:,-1]\n",
    "\n",
    "    ## 存储\n",
    "    new = np.column_stack((data_D, data_L))\n",
    "    new_ = pd.DataFrame(new)\n",
    "    new_.to_csv(path,header=False,index=False)\n",
    "    \n",
    "## 分类数据提取\n",
    "def classify_pre(path='H:\\data\\spec_spi.csv'):\n",
    "    data = pd.read_csv(path,header=None)\n",
    "    data = data.as_matrix()\n",
    "    data_D = data[:,:-1]\n",
    "    data_L = data[:,-1]\n",
    "    data_train, data_test, label_train, label_test = train_test_split(data_D,data_L,test_size=0.9)\n",
    "    return data_train, data_test, label_train, label_test\n",
    "\n",
    "## 分类操作\n",
    "#clf = lgb.LGBMClassifier()\n",
    "clf = SVC()\n",
    "# clf = XGBClassifier(max_depth=20,learning_rate=0.1,n_estimators=200,silent=False,\n",
    "#                       objective='multi:softmax',\n",
    "#                       min_child_weight=1,\n",
    "#                      gamma=0.,\n",
    "#                        scale_pos_weight=1)\n",
    "clf.fit(data_train,label_train)\n",
    "pred = clf.predict(data_test)\n",
    "accuracy = metrics.accuracy_score(label_test, pred)*100\n",
    "print(accuracy)\n",
    "joblib.dump(clf,\"specr_spi.m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
